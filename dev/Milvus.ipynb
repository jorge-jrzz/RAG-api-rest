{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mulvus\n",
    "\n",
    "Mulvus is a simple and efficient similarity search engine. It is designed to search for similar vectors in large collections of high-dimensional vectors.\n",
    "\n",
    "more details of its use with python can be found in the [documentation](https://milvus.io/docs/manage_databases.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "%pip install openai\n",
    "%pip install polars\n",
    "%pip install pymilvus\n",
    "\n",
    "\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import polars as pl\n",
    "from pymilvus import MilvusClient\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class MilvusManager:\n",
    "    def __init__(self, collection_name):\n",
    "        self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.model_name = \"text-embedding-3-small\"\n",
    "        self.milvus_client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "\n",
    "    def create_embeddings(self, text):\n",
    "        response = self.openai_client.embeddings.create(input=text, model=self.model_name)\n",
    "        response_json = json.loads(response.model_dump_json())\n",
    "        embedding = response_json['data'][0]['embedding']\n",
    "        # logger.info(\"Embedding generated for text\")\n",
    "        return embedding\n",
    "    \n",
    "\n",
    "    def create_collection(self):\n",
    "        try:\n",
    "            if self.milvus_client.has_collection(collection_name=self.collection_name):\n",
    "                self.milvus_client.drop_collection(collection_name=self.collection_name)\n",
    "            self.milvus_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                dimension=1536,  # The vectors we will use in this demo has 768 dimensions\n",
    "                auto_id=False, \n",
    "            )\n",
    "            # logger.info(f\"Collection created: {self.collection_name}\")\n",
    "            print(f\"Collection created: {self.collection_name}\")\n",
    "        except Exception as e:\n",
    "            # logger.error(f\"Error creating collection: {e}\")\n",
    "            print(f\"Error creating collection: {e}\")\n",
    "\n",
    "\n",
    "    def insert_points(self, df: pl.DataFrame):\n",
    "        dtf = df.with_columns((pl.col(\"text\").map_elements(self.create_embeddings, return_dtype=pl.List(pl.Float64))).alias(\"vector\"))\n",
    "        data = dtf.to_dicts()\n",
    "        res = self.milvus_client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            data=data\n",
    "        )\n",
    "        print(res)\n",
    "\n",
    "\n",
    "    def search(self, input_text, limit=3) -> str:\n",
    "        search_params = {\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"params\": {\n",
    "                \"radius\": 0.4, # Radius of the search circle\n",
    "                \"range_filter\": 0.5 # Range filter to filter out vectors that are not within the search circle\n",
    "            }\n",
    "        }\n",
    "        query_embedding = self.create_embeddings(input_text)\n",
    "        res = self.milvus_client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            data=[query_embedding],\n",
    "            limit=limit, \n",
    "            search_params=search_params, # Search parameters\n",
    "            output_fields=[\"text\", \"metadata\"], # Output fields to return\n",
    "        )\n",
    "        return json.dumps(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de uso\n",
    "\n",
    "We need the **TextChunk** class to get the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import copy\n",
    "from typing import List, Dict, Optional, Union\n",
    "import polars as pl\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "class TextChunk():\n",
    "\n",
    "    def __init__(self, current_df: Optional[pl.DataFrame] = pl.DataFrame()):\n",
    "        self.current_df = current_df\n",
    "\n",
    "    def __pdf_chunk(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        # Agrupar los elementos por número de página\n",
    "        pages = {}\n",
    "        for item in json_data:\n",
    "            page_number = item['metadata']['page_number']\n",
    "            if page_number not in pages:\n",
    "                pages[page_number] = []\n",
    "            pages[page_number].append(item['text'])\n",
    "        \n",
    "        # Crear una lista de diccionarios con la estructura deseada\n",
    "        data = []\n",
    "        for page_number, texts in pages.items():\n",
    "            data.append({\n",
    "                'metadata': json.dumps({'page_number': page_number, 'filename': json_data[0]['metadata']['filename']}),\n",
    "                'text': ' '.join(texts)\n",
    "            })\n",
    "        \n",
    "        # Crear el DataFrame de Polars\n",
    "        return pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1)\n",
    "    \n",
    "\n",
    "    def __rtf_chunk(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        metadata = {\n",
    "            \"filetype\": json_data[0]['metadata']['filetype'], \n",
    "            \"filename\": json_data[0]['metadata']['filename']\n",
    "        }\n",
    "        text = []\n",
    "\n",
    "        for item in json_data:\n",
    "            text.append(item['text'])\n",
    "\n",
    "        data = {\n",
    "            'metadata': json.dumps(metadata),\n",
    "            'text': ' '.join(text)\n",
    "        }\n",
    "\n",
    "        # Crear el DataFrame de Polars\n",
    "        return pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1) \n",
    "\n",
    "    def __add_if_not_exists(self, new_data: Union[pl.DataFrame, Dict], key_columns: Optional[List]=None) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Agrega nuevas filas al DataFrame si no existen basándose en columnas clave.\n",
    "        \n",
    "        :param df: DataFrame de Polars existente\n",
    "        :param nuevos_datos: DataFrame o diccionario con los nuevos datos\n",
    "        :param columnas_clave: Lista de nombres de columnas para verificar la existencia\n",
    "        :return: DataFrame actualizado\n",
    "        \"\"\"\n",
    "        if key_columns is None:\n",
    "            key_columns = ['metadata', 'text']\n",
    "        # Si nuevos_datos es un diccionario, convertirlo a DataFrame\n",
    "        if isinstance(new_data, dict):\n",
    "            new_data = pl.DataFrame([new_data])\n",
    "        if not isinstance(new_data, pl.DataFrame):\n",
    "            raise TypeError(\"nuevos_datos debe ser un DataFrame de Polars o un diccionario\")\n",
    "        \n",
    "        if self.current_df.is_empty():\n",
    "            self.current_df = self.current_df.vstack(new_data)\n",
    "            return self.current_df\n",
    "\n",
    "        # Crear una expresión para verificar si los datos ya existen\n",
    "        condition = pl.all_horizontal([\n",
    "            pl.col(col).is_in(new_data[col])\n",
    "            for col in key_columns\n",
    "        ])\n",
    "\n",
    "        # Filtrar los datos existentes\n",
    "        existing_data = self.current_df.filter(condition)\n",
    "\n",
    "        # Identificar los datos nuevos\n",
    "        new = new_data.join(\n",
    "            existing_data.select(key_columns),\n",
    "            on=key_columns,\n",
    "            how=\"anti\"\n",
    "        )\n",
    "\n",
    "        # Si hay datos nuevos, agregarlos al DataFrame original\n",
    "        if not new.is_empty():\n",
    "            print(\"Se han encontrado datos nuevos para agregar\")\n",
    "            self.current_df = pl.concat([self.current_df, new], how=\"vertical\")\n",
    "        else:\n",
    "            print(\"No hay datos nuevos para agregar\")\n",
    "        return self.current_df\n",
    "    \n",
    "\n",
    "    def text_chunks_to_dataframe(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        filetype = json_data[0]['metadata']['filetype']\n",
    "        if filetype == \"application/pdf\":\n",
    "            df = self.__pdf_chunk(json_data)\n",
    "        elif filetype == \"text/rtf\":\n",
    "            df = self.__rtf_chunk(json_data)\n",
    "        elif filetype.startswith('text'):\n",
    "            data = copy.deepcopy(json_data)\n",
    "            data[0]['metadata'] = json.dumps(data[0]['metadata'])\n",
    "            df = pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1)\n",
    "\n",
    "        self.__add_if_not_exists(new_data=df)\n",
    "        return self.current_df\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> None:\n",
    "        \"\"\"\n",
    "        Save the current DataFrame to a SQLite database checkpoint.\n",
    "\n",
    "        This method saves the current DataFrame to a SQLite database checkpoint file. If the file already exists, it\n",
    "        will be overwritten.\n",
    "\n",
    "        Parameters:\n",
    "            checkpoint_path (str): The path to the SQLite database checkpoint file.\n",
    "        \"\"\"\n",
    "\n",
    "        conn = sqlite3.connect(checkpoint_path)\n",
    "        temp_df = self.current_df.clone()\n",
    "        temp_df.drop_in_place('id')\n",
    "        temp_df.write_database(table_name=table_name, connection=f\"sqlite:///{checkpoint_path}\", if_table_exists=\"replace\")\n",
    "        conn.close()\n",
    "    \n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> pl.DataFrame:\n",
    "        conn = create_engine(f\"sqlite:///{checkpoint_path}\")\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        self.current_df = pl.read_database(query=query, connection=conn.connect()).with_row_index('id')\n",
    "        return self.current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = MilvusManager(\"collection\")\n",
    "# text = TextChunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = text.load_checkpoint(\"checkpoint.db\")\n",
    "manager.create_collection()\n",
    "manager.create_embeddings(\"Texto para ser convertido en embedding\")\n",
    "manager.insert_points(ddf)\n",
    "print(manager.search(\"No existe informacion de esto.\"))\n",
    "print(manager.search(\"archivo de texto\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'distance': 0.4898538887500763,\n",
      "   'entity': {'metadata': '{\"filetype\": \"application/pdf\", \"filename\": '\n",
      "                          '\"presentation_1.48.16_p.m..pdf\", \"page_number\": 17}',\n",
      "              'text': '—7 ) ESTADO DEL ARTE\\n'\n",
      "                      '49\\n'\n",
      "                      'Microsoft N \\\\\\\\ Reclaim.ai\\n'\n",
      "                      'Copilot ANY\\n'},\n",
      "   'id': 17},\n",
      "  {'distance': 0.4183509051799774,\n",
      "   'entity': {'metadata': '{\"filetype\": \"application/pdf\", \"filename\": '\n",
      "                          '\"presentation_1.48.16_p.m..pdf\", \"page_number\": 3}',\n",
      "              'text': 'INTRODUCCI\\n'\n",
      "                      'actividades laborales y personales.\\n'\n",
      "                      'afin\\n'\n",
      "                      '1© Google Workspace facilita la organización de\\n'\n",
      "                      'Los Modelos de Lenguaje Grande (LLM)\\n'\n",
      "                      'C2 permiten entender\\n'\n",
      "                      ' \\n'\n",
      "                      'y generar lenguaje natural\\n'\n",
      "                      'QSzg) de manera similar a un humano.\\n'\n",
      "                      '— . e ah\\n'\n",
      "                      'La generación mejorada por recuperación ere RN Da\\n'\n",
      "                      '(RAG) es el proceso de optimización de la salida ema , '\n",
      "                      's | me - and psy\\n'\n",
      "                      \"de un LLM We ee ó h'xstof_*_ aki0 of en\\n\"\n",
      "                      'N 4 nine as a ox {0}\\n'\n",
      "                      ', — EEl\\n'\n",
      "                      'e IS o Nd p A\\n'},\n",
      "   'id': 3}]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "data = manager.search(\"Inteligencia artificial\")\n",
    "\n",
    "pprint(json.loads(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"metadata\": \"{\\\\\"filetype\\\\\": \\\\\"application/pdf\\\\\", \\\\\"filename\\\\\": '\n",
      " '\\\\\"presentation_1.48.16_p.m..pdf\\\\\", \\\\\"page_number\\\\\": 17}\", \"text\": '\n",
      " '\"\\\\u20147 ) ESTADO DEL ARTE\\\\n49\\\\nMicrosoft N \\\\\\\\\\\\\\\\ Reclaim.ai\\\\nCopilot '\n",
      " 'ANY\\\\n\"}',\n",
      " '{\"metadata\": \"{\\\\\"filetype\\\\\": \\\\\"application/pdf\\\\\", \\\\\"filename\\\\\": '\n",
      " '\\\\\"presentation_1.48.16_p.m..pdf\\\\\", \\\\\"page_number\\\\\": 3}\", \"text\": '\n",
      " '\"INTRODUCCI\\\\nactividades laborales y personales.\\\\nafin\\\\n1\\\\u00a9 Google '\n",
      " 'Workspace facilita la organizaci\\\\u00f3n de\\\\nLos Modelos de Lenguaje Grande '\n",
      " '(LLM)\\\\nC2 permiten entender\\\\n \\\\ny generar lenguaje natural\\\\nQSzg) de '\n",
      " 'manera similar a un humano.\\\\n\\\\u2014 . e ah\\\\nLa generaci\\\\u00f3n mejorada '\n",
      " 'por recuperaci\\\\u00f3n ere RN Da\\\\n(RAG) es el proceso de optimizaci\\\\u00f3n '\n",
      " \"de la salida ema , s | me - and psy\\\\nde un LLM We ee \\\\u00f3 h'xstof_*_ \"\n",
      " 'aki0 of en\\\\nN 4 nine as a ox {0}\\\\n, \\\\u2014 EEl\\\\ne IS o Nd p A\\\\n\"}']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = manager.search(\"Inteligencia artificial\")\n",
    "points = json.loads(data)\n",
    "context = []\n",
    "for point in points[0]:\n",
    "    context.append(json.dumps(point['entity']))\n",
    "\n",
    "pprint(context)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunks-nbCgvLYC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
